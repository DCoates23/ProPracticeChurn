# ProPracticeChurn

## This is a piece of work created in accordance with the requirements of the BSC (Hons) Data Science Degree Apprenticeship with BPP and involves a synthetic open-source dataset.

### Executive Summary

##### In the financial sector, customer churn is a highly relevant problem across banks as ‘keeping hold of customers is much cheaper for organisations than attempting to recruit new ones’ (Lalwani et al, 2022.) Therefore, it becomes extremely important to use analytical tools to gain an understand on customers and their circumstances and how different factors may make them more likely to exit the bank. Due to this, and a desire to improve coding skills in the technique, a linear regression model was selected to build the model and provide training to bring into the workplace, as the skillset can provide substantial value to the organisation, particularly customer service teams as ‘data can provide warnings about those critically close to churning’ (Gold, 2020.)

#### Another primary objective of this project is to use Exploratory Data Analysis (EDA) to provide visualisations for stakeholders to theoretically at least, quickly gain actionable insights into the raw dataset, compared to having to go through a time-consuming process of going through a spreadsheet in its tabular format and gaining very little information, with a variety of graphs instead being able to contribute to a knowledgeable report and overview.

#### Furthermore, in building the model, it gave opportunities to utilise data engineering skills, data checks to ensure that all records were valid and reliable, so although the model ultimately didn’t lead to a strong R-squared score, it did give a reliable Mean Squared Error (MSE) score, therefore implying that the model could be used as a baseline to be compared with other models and a valid dataset to work with ‘making it a popular dataset for ML repositories’ (Rondina et al, 2023.) The result of this is the ability to discuss quality of data with stakeholders on a technical and non-technical level, and also give coders a blueprint to implement further analysis.



### Data Infrastructure & Tools

#### For this personal Data Science project, the synthetic dataset was sourced from Kaggle as a singular CSV file in a zipped format, with 10,000 rows to analyse and split into the regression model. The Integrated Development Environment (IDE) that was used to build the model and conduct EDA was Visual Studio Code, the preferred method for the local system because of a combination of its syntax highlighting, making clear any mistakes in the structure of the code, as well as the ability to ‘install and import an extraordinary amount of libraries and modules mid-development’ (Saabith et al, 2021) providing a greater opportunity to experiment with different methods.

#### The libraries chosen for this project were varied because of the EDA stage of the script, and the need to add depth to provide visualisations in the regression stage to provide stakeholders with both a score and graph. In order to do this efficiently, the ‘Pandas’ and ‘Numpy’ modules were chosen for their ability to manipulate the CSV file into a readable dataframe in Python that could be analysed and split into a training set and testing set. This was then combined with the main libraries for data analysis and visualisations of ‘Matplotlib’, ‘Pyplot’ and ‘Seaborn’ chosen for their ability in modelling efficiently and visualising changes in results, with Seaborn in particular as a statistical package ‘enhancing the visualisations with built-in themes’ (Kanagachidambaresan & Bharathi, 2024.) The simple and powerful nature of these libraries gave Python an advantage over Tableau for the EDA stage of the project, as after discussing with colleagues it was advantageous to conduct the entire findings all in one place, as the graphs could still be utilised by a customer service team combined with data scientists reviewing the code, while also avoiding issues that may arise from Tableau licencing requirements.


 ### Data Engineering

#### With the dataset being sourced from an open-source website such as Kaggle, it was of vital importance for the execution of the rest of the project that the dataset was checked thoroughly as ‘not all datasets are created equal in completeness and validity’ (Liebchen & Shepperd, 2008.) Initially, the file was loaded into the Python script with important checks involving the structure of the dataset and that it all loaded in correctly, using the features of the pandas library to check the names of the columns and broader mathematical characteristics such as standard deviation to highlight any potential outliers.

#### In these circumstances, the data types were all imported as expected, but the consequences can be catastrophic, for example when attempting to develop a model, if some data types are formatted as strings rather than numeric values, the entire script would fail to run, ‘leading to incompatible parameter errors that are complex to repair’ (Chow et al, 2024.)

#### Additionally, the data was checked for duplicates which are extremely common in customer databases, with IDs and names being common. Processing any duplicates is important for this project and any future work so the decision was made to include it in the code with an f-string, so that when the script is run the output comes in a sentence format that makes it clear for technical and non-technical stakeholders. The final step in this project was to process and manipulate the data so that the relevant columns from the dataframe were grouped together as variables and target variables ready for the analysis and modelling stages that were to follow. This supports Yuan & Lin’s (2006) finding that selecting strong grouped variables can lead to more accurate predictions in regression models, which in this scenario are labelled ‘x’ and ‘y.’


 ### Data Visualisations 

#### A key part of any Data Science project is the ability to utilise coding skills in a way non-technical audiences can understand. Utilising data visualisations in Python is an optimal way to achieve this. The major benefit of Python visualisations is that they can be extremely simple to read and take information from, therefore the focus was on ‘avoiding confusion through several differing colour layers’ (Yigitbasioglu & Velcu, 2012.) The main impact this has on stakeholders is time-saving efficiency as they can take key insights in moments without having to learn to navigate new software or technology.

#### Simplifying communication sometimes has to be done through the code, as the dataset labelled churning in binary terms, because of this it was vital to transform the labels on the x-axis using pyplots ticks function to convert it into a simpler ‘yes’ and ‘no’ for consumers with lower levels of data literacy and allowing maximum uptake from relevant colleagues. Similarly, through the implementation of a legend in the regression line, by engaging with stakeholders it was discovered that it would be beneficial to provide extra clarity on a more complex graph to give viewers an understanding of what they are looking at.

### Data Analytics

#### The strength of the model comes from the Mean Squared Error (MSE) with 0.1562 with it being strong in predicting possibilities in a 0-1 range, however due to the implications of the poor R-Squared score of 0.03423, it may be more beneficial to use it as a comparison with other models. Additionally, some of the results of the EDA reveal interesting findings that can be used to engage stakeholders in pursuit of potential solutions. With the boxplot displaying that customers with higher balances are generally more likely to churn, which has the potential to bring massive financial losses as they take their valuable assets to other banks, due to this, stakeholders could take action by developing more premium services for  affluent customers in a ‘retain, deepen and develop relationship’ (Martensson et al, 2005) manner.

#### Another finding is the age distribution of customers, with an imbalance of customers aged around 25 and 45, therefore a potential ideas for stakeholders to implement would be age-specific products, such as mortgage advice for mid-twenties and future retirement planning for mid-forties. After gaining and reflecting on the results, to improve the reliability of the model and make it more efficient, a switch to more classification-based models such as logistic regression or random forests would be much more beneficial, as Rigatti (2017) suggests that it is much more refined than its predecessors as well as linear regression which was chosen for this initial project.

#### In addition to this, as the visualisation shows, there is a clear imbalance in the dataset between churners and non-churners, this can have a negative impact on the models results because with the ratio of almost 4 in every 5 customers not churning, the training dataset may not have enough evidence of churning characteristics to be able to predict future changes. Academic literature provides suggestions on possible solutions that could be utilised in future iterations of the project, leading with Ganganwar (2012) who suggests resizing training datasets as a way of addressing the imbalance by removing some non-churners from the dataset to make it closer to a 50/50 split. This could align with the possibility of taking advantage of publicly available datasets whether it be Kaggle or BuzzFeed etc. where data could be transformed and joins made between separate ones to make a larger set that is consistent despite changes. This is because there are no regulatory or GDPR obligations that need to be followed with a synthetic dataset unlike customer datasets which involve real people’s personal information.

